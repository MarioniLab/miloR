---
title: "GLMM simulations"
output: html_notebook
---

To road-test and develop the GLMM extension to Milo, we need simulated data where we know the ground-truth and the relevant fixed and random effects. I will 
use a series of different simulations that are based around simulating counts with additional contributions from different factors, such as the randomly 
sampling of individuals that depends on some factor (i.e. random effects), and factors that alter the mean counts (i.e. fixed effects).

I'm still not sure whether to formulate the mixed model as a Gamma-Poisson or a negative binomial + Normally distributed random effects. There are pros and cons 
in favour of either. The dangerous answer is to use both, but that would be a) confusing, and b) an inefficient use of time.  For the moment, and these 
simulations, I'll assume the response variable is generated by a negative binomial process with a normal random effect.

```{r, warning=FALSE, message=FALSE}
library(MASS)
library(Matrix)
library(reshape2)
library(ggplot2)
library(ggthemes)
library(ggsci) 
library(cowplot)
library(glmmTMB)
library(scales)
library(viridis)
```

```{r}
source("~/Documents/Milo2.0/R_scripts/miloR/R/glmm_new.R")
```

I'll start by writing a function that simulates a data set with N observations. This will be useful for evaluating the consistency of our model over repeated runs.

```{r}
SimulateMMData <- function(N, fe.betas, re.sigmas,
                           dispersion, grand.mean, n.fe, n.re,
                           re.levels,
                           fe.levels){
    # create a per-level mean effect for each FE
    if(length(fe.levels) != n.fe){
        stop("List entries need to match number of input fixed effects")
    }
    
    if(length(re.levels) != n.re){
        stop("List entries need to match number of input random effects")
    }
    
    # create the design matrices
    X <- matrix(0L, ncol=n.fe+1, nrow=N)
    X[, 1] <- 1
    colnames(X) <- c("Intercept", names(fe.levels))
    
    Z <- matrix(0L, ncol=n.re, nrow=N)
    
    for(i in seq_len(n.fe)){
        if(fe.levels[[i]] == 1){
            X[, i+1] <- sapply(seq_len(N), FUN=function(B){
                rnorm(1, mean=0, sd=1)
                })
        } else if(fe.levels[[i]] == 2){
            X[, i+1] <- sapply(seq_len(N), FUN=function(B){
                sample(c(0, 1), 1)
                })
            X[, i+1] <- as.factor(X[, i+1])
        }else{
            X[, i+1] <- sapply(seq_len(N), FUN=function(B){
                sample(seq_len(fe.levels[[i]]), 1)
                })
            X[, i+1] <- as.factor(X[, i+1])
        }
    }
    
    # Make categorical effects 0 or 1 (not 1 or 2)
    X[,2] <- X[,2] - 1
    
    for(j in seq_len(n.re)){
        if(re.levels[[j]] == 1){
            Z[, j] <- sapply(seq_len, FUN=function(R){
                rnorm(1, mean=1, sd=1)
            })
        } else{
            Z[, j] <- sapply(seq_len(N), FUN=function(R){
                sample(seq_len(re.levels[[j]]), 1)
            })
            Z[, j] <- factor(Z[, j], levels=c(1:re.levels[[j]]))
        }
    }
    colnames(Z) <- names(re.levels)
    
    # construct the full Z
    random.levels <- sapply(seq_len(length(re.levels)), FUN=function(RX) {
        rx.name <- names(re.levels)[RX]
        paste(rx.name, seq_len(re.levels[[rx.name]]), sep="_")
        }, simplify=FALSE)
    names(random.levels) <- names(re.levels)

    full.Z <- initializeFullZ(Z, random.levels)
    
    # get a combination over random effects 
    # and sample each level from the same ~Normal(0, sigma)
    # note that the variance would be G if we also had random slopes
    re.thetas <- list()
    for(i in seq_len(length(re.levels))){
        i.re <- names(random.levels[i])
        i.levels <- length(random.levels[[i.re]])
        i.re.means <- rnorm(n=i.levels, 0, sd=sqrt(re.sigmas[[i.re]])) # sample a random effect value
        i.re.list <- sapply(seq_len(i.levels), FUN=function(X) i.re.means[X])
        names(i.re.list) <- random.levels[[i.re]]
        re.thetas[[i.re]] <- i.re.list
    }
    
    B <- full.Z %*% unlist(re.thetas)

    # map the fixed effects to mean values
    betas <- c(grand.mean, unlist(fe.betas))
    Beta <- X %*% betas

    i.error <- matrix(data = rnorm(N, mean=0, sd=0.001), ncol = 1)
    
    # construct the y.means equation, depending on desired distribution and FE/RE
    y.means <- exp(Beta + B) 
    y.means <- y.means #+ i.error

    #theta.disp <- (mu.mu/2) - dispersion
    #y.counts <- rnegbin(n=nrow(y.means), mu=exp(unique.fe.df$Beta), theta=1)
    #theta.disp <- (mean(y.means)/2) - dispersion
    y.counts <- rnbinom(N, mu = y.means, size = dispersion)

    sim.data <- data.frame("Mean"=y.means, "Mean.Count"=y.counts)
    sim.data <- do.call(cbind.data.frame, list(sim.data, X, Z))
    print(re.thetas)
    
    return(sim.data)
}

fe.levels <- list("FE1"=2)
re.levels <- list("RE1"=10)

grand.mean <- 2

fe.betas <- list("FE1"=0.25)
re.sigmas <- list("RE1"=0.4)

r.dispersion <- 0.5
set.seed(43)
sim.df <- SimulateMMData(N=1000, fe.betas=fe.betas, re.sigmas=re.sigmas, dispersion=r.dispersion, grand.mean=grand.mean, 
                         n.fe=1, n.re=1, re.levels=re.levels, fe.levels=fe.levels)

sim.df$FE1 <- as.factor(sim.df$FE1)
sim.df$RE1 <- as.factor(sim.df$RE1)

head(sim.df)
```

```{r}
# SimulateMMData <- function(N, fe.betas, re.sigmas,
#                            dispersion, grand.mean, n.fe, n.re,
#                            re.levels,
#                            fe.levels){
#     # create a per-level mean effect for each FE
#     if(length(fe.levels) != n.fe){
#         stop("List entries need to match number of input fixed effects")
#     }
#     
#     if(length(re.levels) != n.re){
#         stop("List entries need to match number of input random effects")
#     }
#     
#     # create the design matrices
#     X <- matrix(0L, ncol=n.fe+1, nrow=N)
#     X[, 1] <- 1
#     colnames(X) <- c("Intercept", names(fe.levels))
#     
#     Z <- matrix(0L, ncol=n.re, nrow=N)
#     
#     for(i in seq_len(n.fe)){
#         if(fe.levels[[i]] == 1){
#             X[, i+1] <- sapply(seq_len(N), FUN=function(B){
#                 rnorm(1, mean=0, sd=1)
#                 })
#         } else if(fe.levels[[i]] == 2){
#             X[, i+1] <- sapply(seq_len(N), FUN=function(B){
#                 sample(c(0, 1), 1)
#                 })
#             X[, i+1] <- as.factor(X[, i+1])
#         }else{
#             X[, i+1] <- sapply(seq_len(N), FUN=function(B){
#                 sample(seq_len(fe.levels[[i]]), 1)
#                 })
#             X[, i+1] <- as.factor(X[, i+1])
#         }
#     }
#     
#     # Make categorical effects 0 or 1 (not 1 or 2)
#     X[,2] <- X[,2] - 1
#     
#     for(j in seq_len(n.re)){
#         if(re.levels[[j]] == 1){
#             Z[, j] <- sapply(seq_len, FUN=function(R){
#                 rnorm(1, mean=1, sd=1)
#             })
#         } else{
#             Z[, j] <- sapply(seq_len(N), FUN=function(R){
#                 sample(seq_len(re.levels[[j]]), 1)
#             })
#             Z[, j] <- factor(Z[, j], levels=c(1:re.levels[[j]]))
#         }
#     }
#     colnames(Z) <- names(re.levels)
#     
#     # construct the full Z
#     random.levels <- sapply(seq_len(length(re.levels)), FUN=function(RX) {
#         rx.name <- names(re.levels)[RX]
#         paste(rx.name, seq_len(re.levels[[rx.name]]), sep="_")
#         }, simplify=FALSE)
#     names(random.levels) <- names(re.levels)
# 
#     full.Z <- initializeFullZ(Z, random.levels)
#     
#     # get a combination over random effects 
#     # and sample each level from the same ~Normal(0, sigma)
#     # note that the variance would be G if we also had random slopes
#     re.thetas <- list()
#     for(i in seq_len(length(re.levels))){
#         i.re <- names(random.levels[i])
#         i.levels <- length(random.levels[[i.re]])
#         i.re.means <- rnorm(n=i.levels, 0, sd=sqrt(re.sigmas[[i.re]])) # sample a random effect value
#         i.re.list <- sapply(seq_len(i.levels), FUN=function(X) i.re.means[X])
#         names(i.re.list) <- random.levels[[i.re]]
#         re.thetas[[i.re]] <- i.re.list
#     }
#     
#     unique.re.df <- as.data.frame(Z)
#     unique.re.df$B <- full.Z %*% unlist(re.thetas)
# 
#     # map the fixed effects to mean values
#     unique.fe.df <- as.data.frame(X)
#     unique.fe.df$Beta <- NA
#     
#     betas <- c(grand.mean, unlist(fe.betas))
#     X_new <- as.matrix(unique.fe.df[ ,1:3])
#     unique.fe.df$Beta <- X_new %*% betas
# 
#     i.error <- matrix(data = rnorm(N, mean=0, sd=0.001), ncol = 1)
#     
#     # construct the y.means equation, depending on desired distribution and FE/RE
#     y.means <- exp(unique.fe.df$Beta + unique.re.df$B) 
#     y.means <- y.means + i.error
# 
#     #theta.disp <- (mu.mu/2) - dispersion
#     #y.counts <- rnegbin(n=nrow(y.means), mu=exp(unique.fe.df$Beta), theta=1)
#     y.counts <- rnbinom(N, mu = y.means, size = dispersion)
# 
#     sim.data <- data.frame("Mean"=y.means, "Mean.Count"=y.counts)
#     sim.data <- do.call(cbind.data.frame, list(sim.data, X, Z))
#     
#     return(sim.data)
# }
# 
# fe.levels <- list("FE1"=2, "FE2"=1)
# re.levels <- list("RE1"=3, "RE2"=5)
# 
# grand.mean <- 2
# 
# fe.betas <- list("FE1"=0.03, "FE2"=0.08)
# re.sigmas <- list("RE1"=0.04, "RE2"=0.07)
# 
# r.dispersion <- 0.5
# set.seed(42)
# sim.df <- SimulateMMData(N=500, fe.betas=fe.betas, re.sigmas=re.sigmas, dispersion=r.dispersion, grand.mean=grand.mean, 
#                          n.fe=2, n.re=2, re.levels=re.levels, fe.levels=fe.levels)
# 
# sim.df$FE1 <- as.factor(sim.df$FE1)
# sim.df$RE1 <- as.factor(sim.df$RE1)
# sim.df$RE2 <- as.factor(sim.df$RE2)
# 
# head(sim.df)
```

By having this wrapped up in a function I can replicate it multiple times to get a series of simulations.

```{r, fig.height=2.55, fig.width=4.95}
sim.melt <- melt(sim.df, id.vars=c("Mean", "Mean.Count", "Intercept", "FE2"))

ggplot(sim.melt, aes(x=value, y=Mean.Count)) +
    geom_boxplot()  +
    theme_cowplot() +
    expand_limits(y=c(0)) + 
    facet_wrap(~variable, scales="free_x") +
    NULL
```

These show the relationship between the factor variables and the simulated counts.

```{r, fig.height=2.55}
ggplot(sim.df, aes(x=FE2, y=Mean.Count)) +
    geom_point() +
    theme_cowplot() +
    expand_limits(y=c(0)) +
    NULL
```

This shows the relationship between the continuous variables and the mean counts. With these data I will compare my implementation to glmmTMB and a straight-up 
ANOVA-mixed model.

## glmmTMB

```{r}
#nb.glm <- glmmTMB(Mean.Count ~ 1 + FE1 + FE2  + (1|RE1) + (1|RE2), data=sim.df, family=nbinom2(link="log"), REML=FALSE, se=TRUE)
nb.glm <- glmmTMB(Mean.Count ~ 1 + FE1  + (1|RE1), data=sim.df, family=nbinom2(link="log"), REML=FALSE, se=TRUE)

summary(nb.glm)
```


## VCA ANOVA

```{r}
require(VCA)
sim.df$log.Counts <- log(sim.df$Mean.Count + 1)
vca.obj <- anovaMM(Mean.Count ~ FE1 + FE2  + (RE1) + (RE2), sim.df)
vca.obj
```


## My model

```{r, warning=TRUE}
set.seed(41) #42

# random.levels <- list("RE1"=paste("RE1", levels(sim.df$RE1), sep="_"),
#                       "RE2"=paste("RE2", levels(sim.df$RE2), sep="_"))
random.levels <- list("RE1"=paste("RE1", levels(sim.df$RE1), sep="_"))

# X <- as.matrix(data.frame("Intercept"=rep(1, nrow(sim.df)), "FE1"=as.numeric(sim.df$FE1) - 1, "FE2"=sim.df$FE2))
# Z <- as.matrix(data.frame("RE1"=as.numeric(sim.df$RE1), "RE2"=as.numeric(sim.df$RE2)))
# y <- sim.df$Mean.Count

X <- as.matrix(data.frame("Intercept"=rep(1, nrow(sim.df)), "FE1"=as.numeric(sim.df$FE1) - 1))
Z <- as.matrix(data.frame("RE1"=as.numeric(sim.df$RE1)))
y <- sim.df$Mean.Count

dispersion <- 0.5

# covariance between u's
glmm.control <- glmmControl.defaults()
# glmm.control$laplace.int <- "fe"
#glmm.control$laplace.int <- "full"
# glmm.control$laplace.int <- "re"
# might need to tweak the loglihood convergence threshold
#glmm.control$likli.tol <- 1e-6
glmm.control$max.iter <- 15
model.list <- runGLMM(X=X, Z=Z, y=y, random.levels=random.levels, glmm.control=glmm.control, dispersion=dispersion)
conv.list <- model.list$Iterations

model.coef <- c(model.list$FE, model.list$RE, model.list$Sigma)
var.comps <- model.list$VarComp
```

```{r}
model.coef
```

Above shows the coefficient and variance component estimates. We can evaluate the model a bit more by looking at the convergence and loglihood across iterations.

```{r}

diff.df <- melt(do.call(cbind,
                        lapply(conv.list, function(X){
                            x <- X$Theta.Diff
                            names(x) <- rownames(X$Theta.Diff)
                            x
                            })))
diff.df$Var2 <- rep(1:(max(as.numeric(names(model.list$Iterations)))), each = 12)
# likli.df <- do.call(rbind.data.frame,
#                     lapply(conv.list, function(X){
#                         X$Loglihood
#                         }))
# colnames(likli.df) <- c("Loglihood")
# likli.df$Iters <- seq_len(nrow(likli.df))

# full.logli.df <- do.call(rbind.data.frame,
#                     lapply(conv.list, function(X){
#                         X$Full.Loglihood
#                         }))
# colnames(full.logli.df) <- c("Full.Loglihood")
# full.logli.df$Iters <- seq_len(nrow(full.logli.df))


theta.df <- melt(do.call(cbind,
                        lapply(conv.list, function(X){
                            x <- X$Theta
                            names(x) <- rownames(X$Theta)
                            x
                            })))
theta.df$Var2 <- rep(1:(max(as.numeric(names(model.list$Iterations)))), each = 12)

variance.diff.df <- melt(do.call(cbind,
                        lapply(conv.list, function(X){
                            x <- X$Sigma.Diff
                            x
                            })))
variance.diff.df$Var2 <- rep(1:(max(as.numeric(names(model.list$Iterations)))), each = 1)

variance.df <- melt(do.call(cbind,
                        lapply(conv.list, function(X){
                            x <- X$Sigma
                            x
                            })))
variance.df$Var2 <- rep(1:(max(as.numeric(names(model.list$Iterations)))), each = 1)

# var.comp.df <- melt(do.call(cbind,
#                             lapply(conv.list, function(X){
#                                 x <- X$Var.Comps
#                                 names(x) <- c(names(random.levels), "residual")
#                                 x
#                                 })))

# diff.var.comp.df <- melt(do.call(cbind,
#                             lapply(conv.list, function(X){
#                                 x <- X$Var.Comp.Diff
#                                 names(x) <- c(names(random.levels), "residual")
#                                 x
#                                 })))
# diff.var.comp.df <- diff.var.comp.df[!is.na(diff.var.comp.df$Var1), ]
```


```{r, warning=FALSE, message=FALSE}
ggplot(likli.df, aes(x=Iters, y=Loglihood)) +
    # geom_point() +
    geom_line() +
    theme_cowplot() +
    labs(x="Iteration", y=expression(bold(paste("log L(",sigma,"; y)")))) +
    # scale_y_log10() +
    NULL
```


```{r, warning=FALSE, message=FALSE}
ggplot(full.logli.df, aes(x=Iters, y=Full.Loglihood)) +
    # geom_point() +
    geom_line() +
    theme_cowplot() +
    labs(x="Iteration", y=expression(bold(paste("log L(",sigma,", ", beta, ", u, ; y)")))) +
    # scale_y_log10() +
    NULL
```



```{r}
theta.conv <- 1e-6

# ggplot(diff.df, aes(x=Var2, y=abs(value), colour=Var1)) +
#     # geom_point() +
#     geom_hline(yintercept=theta.conv, lty=2, col='red') +
#     geom_line() +
#     theme_cowplot() +
#     scale_colour_ptol() +
#     # scale_x_continuous(breaks=c(0:100)) +
#     labs(x="Iteration", y=expression(theta[0] - theta["t"])) +
#     facet_wrap(~Var1, scales="free_y") +
#     #scale_y_log10() +
#     # expand_limits(y=c(1e-8)) +
#     NULL

ggplot(diff.df, aes(x=Var2, y=abs(value), colour=Var1)) +
    # geom_point() +
    geom_hline(yintercept=theta.conv, lty=2, col='red') +
    geom_line() +
    theme_cowplot() +
    scale_colour_ptol() +
    # scale_x_continuous(breaks=c(0:100)) +
    labs(x="Iteration", y=expression(theta[0] - theta["t"])) +
    facet_wrap(~Var1, scales="free_y") +
    #scale_y_log10() +
    # expand_limits(y=c(1e-8)) +
    NULL

ggplot(variance.diff.df[2:nrow(variance.diff.df),], aes(x=Var2, y=abs(value), colour=Var1)) +
    # geom_point() +
    geom_hline(yintercept=theta.conv, lty=2, col='red') +
    geom_line() +
    theme_cowplot() +
    scale_colour_ptol() +
    # scale_x_continuous(breaks=c(0:100)) +
    labs(x="Iteration", y=expression(theta[0] - theta["t"])) +
    facet_wrap(~Var1, scales="free_y") +
    #scale_y_log10() +
    # expand_limits(y=c(1e-8)) +
    NULL
```


```{r, fig.height=3.15, fig.width=8.15}
ggplot(diff.var.comp.df, aes(x=Var2, y=value, colour=Var1)) +
    geom_line() +
    theme_cowplot() +
    scale_colour_npg() +
    labs(x="Iteration", y=expression(sigma[0] - sigma["t"])) +
    facet_wrap(~Var1, scales="free_y") +
    NULL
```

These are the differences in the variance component estimates over the iterations. It looks like they are starting to converge, but are definitely not there yet, especially 
for the second random effect which I would expect to be much small than the first.

```{r, fig.height=3.15, fig.width=8.15}
ggplot(var.comp.df, aes(x=Var2, y=value*100, colour=Var1, group=Var1)) +
    # geom_point() +
    geom_line() +
    theme_cowplot() +
    scale_colour_npg() +
    # scale_x_continuous(breaks=c(0:100)) +
    labs(x="Iteration", y="%Variance") +
    facet_wrap(~Var1, scales="free_y") +
    # expand_limits(y=c(0)) +
    NULL
```

Currently these variance components sum to > total variance, and hence the residual variance is negative!!

```{r, fig.height=5.15, fig.width=10.95}
ggplot(theta.df, aes(x=Var2, y=value, colour=Var1, group=Var1)) +
    # geom_point() +
    geom_line() +
    theme_cowplot() +
    scale_colour_d3("category20c") +
    # scale_x_continuous(breaks=c(0:100)) +
    labs(x="Iteration", y=expression(theta["t"])) +
    facet_wrap(~Var1, scales="free_y") +
    # expand_limits(y=c(0)) +
    NULL

ggplot(variance.df, aes(x=Var2, y=(value), colour=Var1, group=Var1)) +
    # geom_point() +
    geom_line() +
    theme_cowplot() +
    scale_colour_d3("category20c") +
    # scale_x_continuous(breaks=c(0:100)) +
    labs(x="Iteration", y=expression(theta["t"])) +
    facet_wrap(~Var1, scales="free_y") +
    # expand_limits(y=c(0)) +
    NULL
```

Let's also plot the shape of the loglihood over the different parameter values for the variance components.

```{r, fig.height=3.15, fig.width=8.15}
var.loglihood.df <- merge(likli.df, var.comp.df, by.x='Iters', by.y='Var2')
colnames(var.loglihood.df) <- c("Iters", "Loglihood", "VarComp", "VarComp.value")
var.loglihood.df <- merge(var.loglihood.df, theta.df, by.x='Iters', by.y='Var2')

ggplot(var.loglihood.df, 
       aes(x=VarComp.value, y=Loglihood, colour=VarComp, group=Var1)) +
    # geom_point() +
    geom_line() +
    theme_cowplot() +
    scale_colour_npg() +
    # scale_x_continuous(breaks=c(0:100)) +
    labs(x="Param value", y="Loglihood") +
    facet_wrap(~VarComp, scales="free_x") +
    # expand_limits(y=c(0)) +
    NULL
```


```{r, fig.height=6.15, fig.width=8.15}
var.loglihood.df <- merge(full.logli.df, var.comp.df, by.x='Iters', by.y='Var2')
colnames(var.loglihood.df) <- c("Iters", "Loglihood", "VarComp", "VarComp.value")
var.loglihood.df <- merge(var.loglihood.df, theta.df, by.x='Iters', by.y='Var2')

ggplot(var.loglihood.df, 
       aes(x=value, y=Loglihood, colour=Var1, group=Var1)) +
    # geom_point() +
    geom_line() +
    theme_cowplot() +
    scale_colour_ptol() +
    # scale_x_continuous(breaks=c(0:100)) +
    labs(x="Param value", y="Loglihood") +
    facet_wrap(~Var1, scales="free_x") +
    # expand_limits(y=c(0)) +
    NULL
```

Without the Hessian for the variance components we can't estimate any SEs. If only one of the REs is included then the model can be quite unstable, however, 
this might just be how I've set up the simulations as much as the model being problematic.

Now I will need to create a run of simulations and run my GLMM on each of these.

```{r, warning=FALSE, message=FALSE}
runSim <- function(...){
    fe.levels <- list("FE1"=2, "FE2"=1)
    # re.levels <- list("RE1"=3, "RE2"=5)
    re.levels <- list("RE1"=3)
    grand.mean <- 0.5
    
    fe.betas <- list("FE1"=1.25, "FE2"=0.25)
    n.fe <- length(fe.betas)
    # re.sigmas <- list("RE1"=2, "RE2"=4)
    re.sigmas <- list("RE1"=2)
    n.re <- length(re.sigmas)

    r.dispersion <- 2
    sim.df <- SimulateMMData(N=50, fe.betas=fe.betas, re.sigmas=re.sigmas, dispersion=r.dispersion, grand.mean=grand.mean, 
                             n.fe=n.fe, n.re=n.re, re.levels=re.levels, fe.levels=fe.levels)
    
    sim.df$FE1 <- as.factor(sim.df$FE1)
    sim.df$RE1 <- as.factor(sim.df$RE1)
    # sim.df$RE2 <- as.factor(sim.df$RE2)
    
    return(sim.df)
}

n.sims <- 100
sim.list <- replicate(n.sims, runSim(), simplify=FALSE)
```

I've simulated 100 datasets with the same input parameters to get an indication of how well the model deals with this random variation. I have a feeling 
that th model might be a bit over-specified, therefore I'll only test 1 of the random effects at a time.

```{r, warning=FALSE, message=FALSE, error=FALSE}
glmmWrapper <- function(sim.df){
    # random.levels <- list("RE1"=paste("RE1", levels(sim.df$RE1), sep="_"),
    #                       "RE2"=paste("RE2", levels(sim.df$RE2), sep="_"))
    # random.levels <- list("RE2"=paste("RE2", levels(sim.df$RE2), sep="_"))
    random.levels <- list("RE1"=paste("RE1", levels(sim.df$RE1), sep="_"))

    X <- as.matrix(data.frame("Intercept"=rep(1, nrow(sim.df)), "FE1"=as.numeric(sim.df$FE1)-1, "FE2"=sim.df$FE2))
    # Z <- as.matrix(data.frame("RE1"=as.numeric(sim.df$RE1), "RE2"=as.numeric(sim.df$RE2)))
    Z <- as.matrix(data.frame("RE1"=as.numeric(sim.df$RE1)))
    Z <- Z[, c("RE1"), drop=FALSE]
    y <- sim.df$Mean.Count
    
    # covariance between u's
    glmm.control <- glmmControl.defaults()
    # glmm.control$laplace.int <- "fe"
    glmm.control$laplace.int <- "full"
    # might need to tweak the loglihood convergence threshold
    glmm.control$likli.tol <- 1e-6
    try({
        i.start <- proc.time()
        model.list <- runGLMM(X=X, Z=Z, y=y, random.levels=random.levels, glmm.control=glmm.control)
        i.end <- proc.time()
        conv.list <- model.list$Iterations

        out.list <- list("coefficients"=c(model.list$FE, model.list$RE),
                         "varcomps"=model.list$VarComp,
                         "iters"=model.list$Iters,
                         "se"=model.list$SE,
                         "converged"=model.list$converged,
                         "loglihood"=model.list$Loglihood,
                         "dispersion"=model.list$Dispersion,
                         "start"=i.start, "end"=i.end, "elapsed"=(i.end - i.start)[3])
        return(out.list)
        })
}

glmm.sim.list <- lapply(sim.list, glmmWrapper)
table(unlist(lapply(glmm.sim.list, class)))
# glmmWrapper(sim.list[[11]])
```

`r table(unlist(lapply(glmm.sim.list, class)))[2]`% of the runs failed with an error - probably an issue with the vast range of counts. I'll assess the 
other runs for consistency.

```{r, warning=FALSE, message=FALSE, fig.height=2.15, fig.width=4.15}
sim.means <- unlist(lapply(sim.list, FUN=function(CX) mean(CX$Mean.Count)))
sim.vars <- unlist(lapply(sim.list, FUN=function(CX) var(CX$Mean.Count)))
sim.sum <- data.frame("Sim"=seq_len(100), "Mean"=sim.means, "Variance"=sim.vars)
failed.run <- which(unlist(lapply(glmm.sim.list, class)) %in% c("try-error"))
sim.sum$Failed <- sim.sum$Sim %in% failed.run
sim.melt <- melt(sim.sum, id.vars=c("Sim", "Failed"))

ggplot(sim.melt, aes(x=value, fill=variable)) +
    geom_histogram(bins=30) +
    theme_cowplot() +
    facet_wrap(~variable, scales="free") +
    scale_fill_npg() +
    theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1, size=10)) +
    NULL
```

These are the mean and variance within each simulation - there are some with extremely large means and variances - I wonder if these are the simulations 
that the model breaks on. I'll plot the ones where the model ran and did not to double-check this.

```{r}
ggplot(sim.melt, aes(x=value, fill=Failed)) +
    geom_histogram() +
    theme_cowplot() +
    facet_wrap(Failed~variable, scales="free") +
    scale_fill_npg() +
    theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1, size=10)) +
    NULL
```

The lower panels show the simulations that failed - they do generally have a high variance and mean, but it's not clear-cut. What about the mean-variance 
relationship?

```{r, warning=FALSE, message=FALSE, fig.height=2.95, fig.width=4.95}
ggplot(sim.sum, aes(x=Mean, y=Variance, colour=Failed)) +
    geom_abline() + 
    geom_point() +
    theme_cowplot() +
    # scale_y_log10() +
    # scale_x_log10() +
    scale_colour_npg() +
    expand_limits(y=c(0), x=c(0)) +
    NULL
```

This plot shows the mean-variance relationship for each simulation. We can see that the mean is always less than the variance confirming that our simulated 
data are over-dispersed as we wish.

## Parameter distributions

```{r, fig.height=4.95, fig.width=7.95}
getCoeffs <- function(glmm.list, sim){
    data.frame("Param"=names(glmm.list$coefficients), "Coeff"=glmm.list$coefficients, "Sim"=sim)
}

glmm.keep.list <- glmm.sim.list[unlist(lapply(glmm.sim.list, class)) %in% c("list")] # just keep the ones that didn't fail
remain.sims <- length(glmm.keep.list)
glmm.coeff.df <- do.call(rbind.data.frame, sapply(seq_len(remain.sims), FUN=function(X) getCoeffs(glmm.keep.list[[X]], X), simplify=FALSE))

ggplot(glmm.coeff.df, aes(x=Coeff, fill=Param)) +
    geom_histogram(bins=30) +
    theme_cowplot() +
    scale_fill_d3("category20c") +
    facet_wrap(~Param, scales="free") +
    NULL
```

For the most part these parameters are fairly tightly distributed around 0, though the random effects estimates are definitely more variable.

```{r, fig.height=2.15, fig.width=5.15}
getVarComps <- function(glmm.list, sim){
    data.frame("VarComp"=names(glmm.list$varcomps), "Est"=glmm.list$varcomps, "Sim"=sim)
}

glmm.vc.df <- do.call(rbind.data.frame, sapply(seq_len(remain.sims), FUN=function(X) getVarComps(glmm.keep.list[[X]], X), simplify=FALSE))

ggplot(glmm.vc.df, aes(x=Est, fill=VarComp)) +
    geom_histogram(bins=30) +
    theme_cowplot() +
    scale_fill_d3("category20c") +
    facet_wrap(~VarComp, scales="free") +
    theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1, size=9)) +
    scale_x_continuous(limits=c(-2, 2), oob=squish) +
    NULL
```

A couple of things to note here in these distributions of variance components:

1) There are some components that are censored at the boundary (-2, 2) which indicates unstable estimates
2) There are negative variance components, which _should_ be constrained to 0.

How many of these runs have actually converged?

```{r, warning=FALSE, message=FALSE}
getConverge <- function(glmm.list, sim){
    data.frame("Converge"=glmm.list$converge, "Sim"=sim)
}
glmm.conv.df <- do.call(rbind.data.frame, sapply(seq_len(remain.sims), FUN=function(X) getConverge(glmm.keep.list[[X]], X), simplify=FALSE))

table(unlist(lapply(glmm.keep.list, FUN=function(X) X$converged)))
```

This suggests that most of the models have not converged - how quickly do they reach convergence then?

```{r}
getIters <- function(glmm.list, sim){
    data.frame("Iters"=glmm.list$iters, "Sim"=sim, "Converge"=glmm.list$converged)
}

glmm.iter.df <- do.call(rbind.data.frame, sapply(seq_len(remain.sims), FUN=function(X) getIters(glmm.keep.list[[X]], X), simplify=FALSE))
glmm.iter.df <- dplyr::filter(glmm.iter.df, Converge == TRUE)
barplot(table(unlist(lapply(glmm.keep.list, FUN=function(X) X$iters))))
```

It seems that most runs will generally converge in < 10 iterations, but there is also a substantial portion that hit the 100 max; these are the runs that 
don't converge.  I wonder if this has anything to do with determining the dispersion parameter? I have sneaky feeling that I have used Var[Y] instead of 
Var[Y|b] to compute the dispersion.

```{r, fig.height=2.95, fig.width=3.95}
getDispersion <- function(glmm.list, sim){
    data.frame("Disp"="R", "Val"=glmm.list$dispersion, "Sim"=sim)
}

glmm.disp.df <- do.call(rbind.data.frame, sapply(seq_len(remain.sims), FUN=function(X) getDispersion(glmm.keep.list[[X]], X), simplify=FALSE))

ggplot(glmm.disp.df, aes(x=Val, fill=Disp)) +
    geom_histogram(bins=30) +
    theme_cowplot() +
    scale_fill_d3("category20c") +
    facet_wrap(~Disp, scales="free") +
    theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1, size=9)) +
    # scale_x_continuous(limits=c(-2, 2), oob=squish) +
    NULL
```

This doesn't look too bad, most of the dispersion estimates are in the [0, 1] range which seems fairly sensible. Are there any relations between the 
dispersion estimates and  failure to converge/out of range variance components?


```{r, warning=FALSE, message=FALSE}
all.param.df <- Reduce(x=list(glmm.disp.df, glmm.vc.df, glmm.coeff.df, glmm.iter.df, glmm.conv.df),
                       f=function(x, y) merge(x, y, by='Sim'))
```


```{r, warning=FALSE, message=FALSE, fig.height=2.95, fig.width=4.15}
ggplot(all.param.df, aes(x=Converge, y=Val, fill=Disp)) +
    geom_boxplot(width=0.5) +
    theme_cowplot() +
    scale_fill_d3("category20c") +
    NULL
```

The model runs that fail to converge have a marginally higher dispersion; is this a clue to samples with very high variances being a problem?

```{r, warning=FALSE, message=FALSE, fig.height=2.95, fig.width=4.15}
ggplot(all.param.df, aes(x=Converge, y=Est, fill=VarComp)) +
    geom_boxplot(width=0.5) +
    theme_cowplot() +
    scale_fill_d3("category20c") +
    # scale_y_continuous(limits=c(-1, 1), oob=squish) +
    NULL
```

OK, so the model runs that fail to converge almost all have out-of-range variance components. That does indicate a stability issue. There are only 50 
observations in these simulations, so this might be part of the problem. Are the fixed effect parameters also affected?

```{r, warning=FALSE, message=FALSE, fig.height=2.95, fig.width=6.15}
ggplot(all.param.df, aes(x=Converge, y=Coeff, fill=Param)) +
    geom_boxplot(width=0.5) +
    theme_cowplot() +
    scale_fill_d3("category20c") +
    # scale_y_continuous(limits=c(-10, 10), oob=squish) +
    NULL
```

This goes to show that the random effects estimation is definitely unstable, so when the model fails to converge it's because the random effects are really 
skewed - this also goes for the intercept. I'm not super confident if it's a chicken or egg situation, i.e., are the variance components the problem or the 
random effect estimation? If I change the dispersion computation, then this will at least give me an indication of whether or not it makes a difference.

Now what about the timing? This will be important when run over 1000's of neighbourhoods.

```{r, fig.height=2.95, fig.width=3.95}
getTime <- function(glmm.list, sim){
    data.frame("Elapsed"=glmm.list$elapsed, "Sim"=sim)
}

glmm.time.df <- do.call(rbind.data.frame, sapply(seq_len(remain.sims), FUN=function(X) getTime(glmm.keep.list[[X]], X), simplify=FALSE))

ggplot(glmm.time.df, aes(x=Elapsed)) +
    geom_histogram(bins=30) +
    theme_cowplot() +
    theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1, size=9)) +
    # scale_x_continuous(limits=c(-2, 2), oob=squish) +
    NULL
```

This looks like the majority of models run in < 1 second, but some take a lot longer. I'll need to look at how this scales with the sample size.

```{r, warning=FALSE, message=FALSE}
all.param.df <- Reduce(x=list(glmm.disp.df, glmm.vc.df, glmm.coeff.df, glmm.iter.df, glmm.conv.df, glmm.time.df),
                       f=function(x, y) merge(x, y, by='Sim'))
```



```{r, warning=FALSE, message=FALSE, fig.height=2.95, fig.width=6.15}
ggplot(all.param.df, aes(x=Converge, y=Elapsed)) +
    geom_boxplot(width=0.5) +
    theme_cowplot() +
    # scale_fill_d3("category20c") +
    # scale_y_continuous(limits=c(-5, 5), oob=squish) +
    expand_limits(y=c(0)) +
    NULL
```

Perhaps predictably, the models that converge are the fastest to run - we will really need to figure out what determines if the model fails or not...

```{r}
getSE <- function(glmm.list, sim){
    data.frame("Param"=names(glmm.list$se), "SE"=glmm.list$se,  "Sim"=sim)
}

glmm.se.df <- do.call(rbind.data.frame, sapply(seq_len(remain.sims), FUN=function(X) getSE(glmm.keep.list[[X]], X), simplify=FALSE))
all.param.df <- Reduce(x=list(glmm.disp.df, glmm.vc.df, glmm.coeff.df, glmm.iter.df, glmm.conv.df, glmm.time.df),
                       f=function(x, y) merge(x, y, by='Sim'))
all.param.df <- merge(all.param.df, glmm.se.df, by=c("Sim", "Param"))
all.param.df$LCI <- all.param.df$Coeff - (1.96 * sqrt(all.param.df$SE))
all.param.df$UCI <- all.param.df$Coeff + (1.96 * sqrt(all.param.df$SE))
```



```{r, warning=FALSE, message=FALSE, fig.height=5.95, fig.width=8.95}
ggplot(all.param.df, aes(y=SE, fill=Converge)) +
    geom_histogram(bins=30) +
    theme_cowplot() +
    scale_fill_d3("category20c") +
    facet_wrap(~Param, scales="free") +
    # scale_y_continuous(limits=c(0, 1e-7), oob=squish) +
    NULL
```

This does look like the SE are much bigger for the runs that fail to converge. These SEs also seem extremely small...

    
