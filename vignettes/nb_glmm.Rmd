---
title: "Estimating equations for a NB-GLMM"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

This notebook will keep a record of the various equations, functions and derivatives required to build a functioning negative binomial GLMM.

## The negative binomial likelihood function in exponential family form

We have the likelihood function $L(\theta;y)$ for $\theta = [p, r]$

$$L(p, r; y) = {y-1\choose r-1} (1-p)^{y-r}p^r $$

We want to find the parameterisation of this function for both mean estimation and in exponential family form so that we can frame this as a GLM(M). An exponential family 
distribution has the form:

$$ f(y| \theta) = h(y)\mbox{exp}\{T(y)\eta(\theta) - A(\theta)\} $$

where $T(y)$ is the sufficient statistic, $h(y)$ is a non-negative function of $y$, $\eta(\theta)$ is the natural parameter and $A(\theta)$ is the cumulant generating function, 
and $\theta$ is the canonical parameter. We can also frame this in terms of the natural parameter:

$$ f(y| \theta) = h(y)\mbox{exp}\{T(y)\eta - A(\eta)\} $$

Alternatively, we can frame this as in Nelder _et al._:

$$ f(y|\theta) = \mbox{exp} \bigg[ \frac{t(y)\theta - b(\theta)}{a(\phi)} + c(y, \phi) \bigg]$$

Where $t(y)$ is the sufficient statistic, $\theta$ is the canonical parameter, $b(\theta)$ is the cumulant generating function in the canonical parameter form and c(y, $\phi$) is 
rescaled equivalent of $h(y)$. The new parameter $\phi$ is a scale parameter for the exponential family distribution.

For the negative binomial distribution we can't usually define the likelihood function $L(p, r; y)$ as an exponential family form. However, if we treat $r$ as a known value, then 
we can as follows:

$$L(p; r, y) = \mbox{exp}\bigg( \underbrace{ylog[1-p]}_{t(y)\theta} - \overbrace{(rlog[1-p] - rlogp)}^{b(\theta)} + \underbrace{log\big[\frac{\Gamma(y)}{\Gamma(r)}\big]}_{c(y, \phi)} \bigg)$$

From this we know that $\theta=log(1-p)$. Therefore $1-p=e^{\theta}$ and $p=1-e^{\theta}$. What we don't know is the value for $a(\phi)$.  Importantly, using this 
formulation and the moment generating function we know that $E[Y]=\mu=\frac{r}{p}$, which can also be found because $\frac{d b(\theta)}{d \theta} = E[Y] = \mu$ and 
$Var[Y]=\phi \frac{d^2b(\theta)}{d\theta^2}$.

### Finding the mean and variance

$$b(\eta) = rlog[e^\theta] - rlog[1-e^\theta]$$
Let $u=e^\theta$, $v=rlogu$ $a=1-e^\theta$ and $b=-rloga$ then using the sum rule and the chain rule:

$$E[Y]=\mu=\frac{db(\theta)}{d\theta}=(\frac{du}{d\eta}) (\frac{dv}{du}) + (\frac{da}{d\eta}) (\frac{db}{da})= [e^\theta \cdot \frac{r}{e^\theta}] + [-e^\eta \cdot -{\frac{r}{1-e^\theta}}] = r + \frac{re^\theta}{1-e^\theta}$$

Substituting $1-e^\theta = p$ and $e^\theta = 1-p$

$$E[Y]=\mu=\frac{db(\theta)}{d\theta}=r+(1-p) \cdot r(p)^{-1} = \frac{r(1-p)}{p}+r$$

Expanding out the brackets and simplifying we end up with:

$$E[Y]=\mu=\frac{db(\theta)}{d\theta}=\frac{r-pr}{p} + r = r + \frac{r}{p} - \frac{pr}{p} = r + \frac{r}{p} - r = \frac{r}{p}$$

Thus $E[Y]=\mu=\frac{r}{p}$. Using this we can also find the variance ($Var[Y]$) and the value for $\phi$ as follows (using the identity $p=1-e^\theta$):

$$Var[Y]=\phi\frac{d^2b(\theta)}{d\theta^2} = \phi\frac{d(db(\theta)/d\theta)}{d\theta} = \phi\frac{d\mu}{d\theta}=\phi\frac{d\frac{r}{p}}{\theta}=\phi\frac{d\frac{r}{1-e^\theta}}{d\theta}$$

As we have a quotient, we can use the quotient rule to find this derivative, where the quotient rule is:

$$\frac{df}{dt}= \frac{v\frac{du}{dt} - u\frac{dv}{dt}}{v^2}$$

Therefore, let $u=re^\theta$ and $v=1-e^\theta$, with $\frac{du}{d\theta} = re^\theta$ and $\frac{dv}{d\theta}=-e^\theta$ to get:

$$Var[Y]=\phi\frac{d\frac{r}{1-e^\theta}}{d\theta} = \frac{(1-e^\theta) \cdot re^\theta - (re^\theta \cdot -e^\theta)}{(1-e^\theta)^2} $$

Expanding brackets and simplifying:

$$Var[Y]=\phi\frac{d\frac{r}{1-e^\theta}}{d\theta} = \frac{re^\theta -r(e^\theta)^2 + r(e^\theta)^2}{(1-e^\theta)^2} = \frac{re^\theta}{(1-e^\theta)^2} $$

Replacing $e^\theta=1-p$ and $1- e^\theta=p$:

$$Var[Y]=\phi\frac{d\frac{r}{1-e^\theta}}{d\theta} = \frac{r(1-p)}{p^2} $$
### Finding the first and second moments

Using the first and second derivatives of the moment generating function ($M(t)$) of the negative binomial we can equate these two values to solve for $\phi$. Evaluating the 
first and second derivatives $M'(t)$ and $M''(t)$ at $t=0$ gives us the first and second moments, respectively.

$$M(t)=\frac{(pe^t)^r}{[1-(1-p)e^t]^r}$$

Evaluating $M(0)$ will give the 0th moment, which is 1.

$$M'(t)=\frac{[1-(1-p)e^t]^r pe^t r(pe^t)^{r-1}-(pe^t)^r r[1-(1-p) e^t]^{r-1}\cdot-(1-pe^t)}{[1-(1-p)e^t]^{2r}}$$

Evaluating $M'(t)$ at $t=0$ gives the first moment $E[Y]$:

$$E[Y] = M'(0)=\frac{[1-(1-p)e^0]^r pe^0 r(pe^0)^{r-1}-(pe^0)^r r[1-(1-p) e^0]^{r-1}\cdot-(1-pe^0)}{[1-(1-p)e^0]^{2r}}\\ = \frac{[1-(1-p)]^r p r(p)^{r-1}-(p)^r r[1-(1-p) ]^{r-1}\cdot-(1-p)}{[1-(1-p)]^{2r}}$$

Simplifying $(1-(1-p)) = p$ and the product of 2 negatives, and separating summed/differences of powers of $p$, then factoring out $p^rrp^rp^{r-1}$:

$$E[Y] = M'(0)= \frac{p^r p rp^rp^{-1}+p^r rp^rp^{-1}(1-p)}{p^{2r}}\\ = \frac{p^rrp^rp^{-1}(p+1-p)}{p^{2r}}\\ = \frac{p^rrp^rp^{-1}}{p^{2r}}$$

Finally cancelling out terms in the numerator and denominator (note: $p^{2r}=p^rp^r$):

$$E[Y] = M'(0) = \frac{p^rrp^rp^{-1}}{p^rp^r}\\ = rp^{-1} \\ = \frac{r}{p}$$

This agrees with our definition of the mean above. To get $Var[Y]$ we need the second moment $E[Y^2]$ because $Var[Y] = E[Y^2] - E[Y]^2$, which is the difference between the 
second moment and the squared mean. Therefore, we need the second derivative $M''(t)$, which is complicated so the derivation will be omitted.

$$M''(t) = r(pe^t)^r(-r-1)[1-(1-p)e^t]^{-r-2}[-(1-p)e^t]+r^2(pe^t)^{r-1}pe^t[1-(1-p)e^t]^{-r-1}$$

We evaluate $M''(t)$ at $t=0$ (I'll leave out the obvious evaluation $e^0=1$ and $(1-(1-p))=p$. Then do some cancelling of negative term products and rearranging,

$$E[Y^2] = M''(0) = rp^r(-r-1)p^{-r-2}[-(1-p)]+r^2p^{r-1}pp^{-r-1} \\ = rp^r(-1)(r+1)p^{-r-2}(-1)(1-p)+r^2p^{r-1}pp^{-r-1}$$

Cancelling and collecting like terms, then expanding brackets:

$$E[Y^2] = M''(0) = rp^r(r+1)p^{-r-2}(1-p)+r^2p^{r-1}pp^{-r-1}\\ = rp^{-2}(r+1)(1-p)+r^2p^{-1}\\ = rp^{-2}(r+1)(1-p) + rp) \\ =rp^{-2}[r-rp+1-p+rp] \\=rp^{-2}[r+(1-p)] \\=\frac{r[r+(1-p)]}{p^2}$$

Now we can compute $Var[Y] = E[Y^2] - E[Y]^2$ and equate it with our variance definition above $\phi\frac{d\frac{r}{1-e^\theta}}{d\theta} = \frac{r(1-p)}{p^2}$.

$$Var[Y] = E[Y^2] - E[Y]^2 = \frac{r[r+(1-p)]}{p^2} - \bigg(\frac{r}{p} \bigg)^2 \\ = \frac{r[r+(1-p)] - r^2}{p^2} \\ = \frac{r^2 + r(1-p)-r^2}{p^2} \\ = \frac{r(1-p)}{p^2}$$

Therefore, we can trivially see that $\phi=1$.

## A mean parameterisation of the Negative Binomial

Now that we know $E[Y]=\mu$ and $Var[Y]$, we can re-parameterise our CDF and likelihood function in terms of the mean, which will allow us to frame our NB-GLMM. Noting 
the following relations,

$$E[Y] = \mu=\frac{r}{p} \\ p=\frac{r}{\mu} \\ 1-p=1-\frac{r}{\mu} \\ Var[Y]= \xi = \frac{r(1-p)}{p^2} = \frac{\mu^2}{r} - \mu \\ r=p\mu = \frac{\mu^2}{\xi+\mu}$$

we can re-write $f(p; y, r)$ as $f(\mu; y, r)$:

$$f(p; y, r) = {y-1\choose r-1} (1-p)^{y-r}p^r \\ f(\mu; y, r) = {y-1 \choose r-1}(1- \frac{r}{\mu})^{y-r}(\frac{r}{\mu})^r$$

To frame our CDF/likelihood interms of an exponential family, it is easier to use the log-likelihood (loglihood):

$$log f(\mu; y, r) = \ell(\mu; y, r) = log \bigg[\frac{\Gamma(y)}{\Gamma(r)} \bigg] + ylog \bigg[1-\frac{r}{\mu} \bigg] - rlog \bigg[1-\frac{r}{\mu} \bigg] + rlogr - rlog\mu$$

From this we can determine the following in terms of our exponential family form:

$$b(\theta) = rlog\bigg[1 - \frac{r}{\mu} \bigg] - rlogr + rlog\mu \\ t(y) = y \\ \theta = log[1- \frac{r}{\mu}]  \\ c(y) = log \bigg[ \frac{\Gamma(y)}{\Gamma(r)} \bigg] \\ e^\theta = 1 - \frac{r}{\mu} \\ 1-e^\theta = \frac{r}{\mu} \\ \mu=\frac{r}{1-e^\theta}$$

and in exponential family form:

$$ f(y, \theta, \phi) =exp \bigg\{ \frac{\overbrace{y}^{t(y)} \overbrace{log \bigg[1-\frac{r}{\mu} \bigg]}^{\theta} - \overbrace{(rlog\bigg[1 - \frac{r}{\mu} \bigg] - rlogr + rlog\mu)}^{b(\theta)} }{\phi} + \overbrace{log \bigg[\frac{\Gamma(y)}{\Gamma(r)} \bigg]}^{c(y, \phi)} \bigg\} \\ =exp \bigg\{ \frac{\overbrace{y}^{t(y)} \overbrace{log \bigg[1-\frac{r}{\mu} \bigg]}^{\theta} - \overbrace{rlog\bigg[1 - \frac{r}{\mu} \bigg] + rlogr - rlog\mu)}^{b(\theta)} }{\phi} + \overbrace{log \bigg[\frac{\Gamma(y)}{\Gamma(r)} \bigg]}^{c(y, \phi)} \bigg\}$$

Of course, we won't have a single observation, but a sequence of samples $Y=\{y_1, y_2, ..., y_n\}$. So our likelihood is actually a product over these separate samples 
(assuming that they are conditionally independent and identically distributed). It turns out that a product of exponential family distributions is itself also in the 
exponential family. So our likelihood and loglihood become:

$$L(\theta; Y,r) = \prod_{i=1}^{n} {y_i-1 \choose r-1}(1- \frac{r}{\mu})^{y_i-r}(\frac{r}{\mu})^r \\ \ell(\theta; Y, r) = \sum_{i=1}^{n} y_i log \bigg[1-\frac{r}{\mu} \bigg] - rlog\bigg[1 - \frac{r}{\mu} \bigg] + rlogr - rlog\mu + log \bigg[\frac{\Gamma(y_i)}{\Gamma(r)} \bigg]$$

We can also cheat a little and replace and $\sum_{i=1}^{n} y_i=n\bar{y}$:

$$\ell(\theta; Y, r) =  n\bar{y} log \bigg[1-\frac{r}{\mu} \bigg] - nrlog\bigg[1 - \frac{r}{\mu} \bigg] + nrlogr - nrlog\mu + \sum_{i=1}^{n}log \bigg[\frac{\Gamma(y_i)}{\Gamma(r)} \bigg] $$


## Building a GLMM

There are several components that we need to construct a GLMM, namely: (1) A linear predictor, (2) a link function to connect the mean value to the linear predictor, (3) 
a sampling distribution for the observations conditional on the random effects, and (4) a sampling distribution for the random effects.

Linear predictor: $\eta=X\beta + Zb$ \
Link function: $g(\theta) = log(\theta)$ \
Inverse link function: $g^{-1}(\eta) = exp(\eta)$ \
Conditional distribution: $y|b \sim NegativeBinomial(\frac{r}{\mu}, r)$ \
Random effect distribution: $b \sim Normal(0, \sigma^2)$

To perform parameter estimation and likelihood evaluation we need to construct the full expanded likelihood (treating r as a known constant).

$$f(y, b, \mu) = \int NegativeBinomial(\frac{r}{\mu}, r) \ Normal(0, \sigma^2) \ \mathbf {db}\\ log \ f(y, b, \mu) =\int ylog \bigg[1-\frac{r}{\mu} \bigg] - rlog\bigg[1 - \frac{r}{\mu} \bigg] + rlogr - rlog\mu + log \bigg[\frac{\Gamma(y)}{\Gamma(r)} \bigg] -(\frac{1}{2})log2\pi - \frac{1}{2}log \sigma^2 - \frac{1}{2}(\frac{b}{\sigma})^2 \ db$$

This is a pretty horrible integral, especially because $\mu=exp(\eta) = exp(X\beta + Zb)$, and for >1 random effect this becomes a high dimensional integral - nasty. 
Therefore, we will need to approximate this extended loglihood and use an iterative algorithm to estimate our parameters. For ease of manipulation, we can also write this 
scalar loglihood in matrix form.

$$ \ell(y, b, \mu) = \int n\bar{y} log \bigg[1-\frac{r}{\mu} \bigg] - nrlog\bigg[1 - \frac{r}{\mu} \bigg] + nrlogr - nrlog\mu + \sum_{i=1}^n log \bigg[\frac{\Gamma(y_i)}{\Gamma(r)} \bigg] -(\frac{c}{2})log2\pi - \frac{1}{2}log (|G|) - \frac{1}{2}b^TG^{-1}b \ db $$

### Parameter estimation with Newton-Raphson

We will use the NR algorithm to approximate our paramaters $\theta=[\beta \ b]$. The algorithm uses a second-order Taylor series expansion around $\theta_0$, which is then 
used to iteratively update the parameter estimates until they reach some convergence criteria: $|\theta_{i-1} - \theta_i| < \epsilon$. Recalling a second-order Taylor series 
expansion of a function $f(x)$ (assuming $f(x)$ is differentiable at $x_0$:

$$f(x) = f(x_0) + \frac{df(x_0)/dx}{1!}(x-x_0) + \frac{d^2f(x_0)/dx^2}{2!}(x-x_0)^2 $$

The first derivative of a likelihood w.r.t. a parameter is the score function $S(\theta)$, which is the tangent line to the loglihood function. In MLE terms we say that the 
likelihood is maximised (or -loglihood minimized), when $S(\theta)=0$. The second derivative is the curvature, which can also be thought of as the variance around the parameter 
estimates, and for multiple parameters is called the Hessian matrix $H(\theta)$, as it also represents the covariance between parameter estimates.

The NR algorithm is written symbolically as:

$$ \theta_{i+1} = \theta_i - \frac{df(\theta)/d\theta}{d^2f(\theta)/d\theta^2} \\  \theta_{i+1} = \theta_i - \frac{1}{d^2f(\theta)/d\theta^2}\frac{df(\theta)}{d\theta} \\$$

When dealing with multiple parameter values we can write this in terms of a score _vector_ and a _matrix_ of second partial derivatives:

$$\theta_{i+1} = \theta_i - H(\theta)^{-1}S(\theta)$$

Therefore, to estimate our $\beta$ and $b$ we need to find the the first and second derivatives:

$$S(\beta) = \frac{\partial \ell(\theta; y, r)}{\partial \beta} \\ S(b) = \frac{\partial \ell(\theta; y, r)}{\partial b} \\ H(\beta)=\frac{\partial^2\ell(\theta; y, r)}{\partial\beta_j\partial\beta_k} \\ H(b)=\frac{\partial^2\ell(\theta; y, r)}{\partial b_l \partial b_q}$$

To compute these involves finding the various partial derivatives, and extensive use of the chain rule. I won't give all of the calculations as they are laborious, but I'll 
provide some of the symbol calculations to aid working backwards from each to $\ell(\theta; y, r)$. Using $\ell(\theta)$ as a short cut to $\ell(\theta; y, r)$:

$$S(\beta) = \frac{\partial \ell(\theta)}{\partial \theta} \frac{\partial \theta}{\partial \mu} \frac{\partial \mu}{\partial \eta} \frac{\partial \eta}{\partial \beta} \\ S(b) = \frac{\partial \ell(\theta)}{\partial \theta} \frac{\partial \theta}{\partial \mu} \frac{\partial \mu}{\partial \eta} \frac{\partial \eta}{\partial b}$$

For convenience we have some of the easier partial derivatives in (mostly) scalar form, based on a number of identities:

$$\eta=X\beta + Zb \\ \frac{\partial \eta}{\partial \beta} = X \\ \frac{\partial \eta}{\partial b} = Z \\ \mu = e^\eta \\ \frac{\partial \mu}{\partial\eta} = e^\eta=\mu \\ \theta = 1- r\mu^{-1} \\  Var(\mu)^{-1} = Var[Y|b]^{-1}=(\frac{\mu^2}{r} - \mu)^{-1} \\ \frac{\partial \ell(\theta)}{\partial \theta} = y - \frac{\frac{\partial b(\theta)}{\partial \theta}}{a(\phi)} = y - \frac{\partial b(\theta)}{\partial \theta} \\ \frac{\partial b(\theta)}{\partial \theta} = n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)$$

Because we are invariably using vectors of parameters, where a scalar is a 1-component vector, we can write some of these in matrix notation:

$$\frac{\partial \mu}{\partial\eta} = \underset{n\times n}{diag}(\mu_i) = D \\ \underset{n \times n}{diag}(\frac{\mu_{i}^2}{r} - \mu_{i}) = V_0 \\ \frac{\partial \theta}{\partial \mu} = Var[Y] = V_0 + DZGZ^TD = V$$

That is, $D$ is a diagonal matrix of the individual means $\mu_i$ and $V_0$ is a diagonal matrix of the variances, and $V_0^{-1}$ is its inverse. Using the 
result from Breslow and Clayton (1993) equation 17, we define $V$.

Putting all of these components 
together with $\frac{\partial \ell(\theta)}{\partial \theta}$ leads to the following score equation vectors:

$$S(\beta) = X^TDV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)] \ : \ k \times 1 \ \text{vector} \\ S(b) = Z^{T}DV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)] - \frac{1}{2}tr(G^{-1}\frac{\partial G}{\partial b}) - G^{-1}b : \ c \times 1 \ \text{vector}$$

Here $G=Var(b)$, the covariance of $b$ which needs to be estimated, or can we use the plug-in estimator of the sample variance $\frac{1}{c} bb^T$ (the matrix equivalent of 
$\hat S = \sum_{j=1}^q (b -\bar b)^2$, where $\bar b=0$ because $b \sim Normal(0, \sigma^2)$)? This also involves the partial derivatives $\frac{\partial G}{\partial u}$, which 
need to be computed for $u \in \{1, 2, ..., c \}$.

Taking the second derivatives we find the Hessian matrices for $\beta$:

$$H(\beta) = X^TDV^{-1}BX + X^TDWDBX + X^TDV^{-1}QDX : \ k \times k \ \text{matrix} \\W_0 = \frac{\partial V_0^{-1}}{\partial \mu} =\underset{n \times n}{diag}\big(\frac{\mu^2(1-2\mu)}{r} - \frac{2\mu^3}{r^2} - \mu\big)  \\  W=\frac{\partial V^{-1}}{\partial \mu} = W_0 + ZGZ^TD + DZGZ^T \\ B = \underset{n \times n}{diag}(y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)) \\  Q=\underset{n \times n}{diag} (-n\big(1 + \frac{r^2}{\mu^2(1-r\mu^{-1})^2} \big))$$

and for $b$:

$$H(b) =  Z^{T}DV^{-1}BZ + Z^{T}DWDBZ + Z^{T}DV^{-1}QDZ - \frac{1}{2} C - G^{-1} : \ c \times c \ \text{matrix} \\ W_0 = \frac{\partial V_0^{-1}}{\partial \mu} =\underset{n \times n}{diag}\big(\frac{\mu^2(1-2\mu)}{r} - \frac{2\mu^3}{r^2} - \mu\big)  \\  W=\frac{\partial V^{-1}}{\partial \mu} = W_0 + ZGZ^TD + DZGZ^T  \\ B = \underset{n \times n}{diag}(y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)) \\ Q=\underset{n \times n}{diag} (-n\big(1 + \frac{r^2}{\mu^2(1-r\mu^{-1})^2} \big)) \\ C = \underset{c \times c}{diag} \big(tr \big[(G^{-1} \frac{\partial G}{\partial b_i})(G^{-1}\frac{\partial G}{\partial b_j}) \big] \big)$$

Therefore, each step of the NR iteration is:

$$\hat\beta = \tilde\beta - H(\beta)^{-1}S(\beta) \\ \hat\beta = \tilde\beta - [X^TDV^{-1}BX + X^TDWDBX + X^TDV^{-1}QDX]^{-1} \bigg[X^TDV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)]\bigg]\\ \hat{b} = \tilde b - H(b)^{-1}S(b) \\ \hat{b} = \tilde b - \big[ Z^{T}DV^{-1}BZ + Z^{T}DWDBZ + Z^{T}DV^{-1}QDZ - \frac{1}{2} C - G^{-1} \big]^{-1}\bigg[ Z^{T}DV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)] - \frac{1}{2} tr(G^{-1} \frac{\partial G}{\partial b}) - G^{-1}b \bigg]$$

We can also define the full Hessian for $\theta = [\beta \ b]$, which I think might mean that we can compute the updates jointly. Therefore, we can define the full vector of 
score equations and Hessian as:

$$\theta = [\beta \ b] \\ S(\theta) = \begin{bmatrix} X^TDV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)] \\ Z^{T}DV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)] - \frac{1}{2} tr(G^{-1} \frac{\partial G}{\partial b}) - G^{-1}b \end{bmatrix} \\ H(\theta) = \left[\begin{array}{c|c} X^TDV^{-1}BX + X^TDWDBX + X^TDV^{-1}QDX & X^TDV^{-1}BZ + X^TDWDBZ + X^TDV^{-1}QDZ \\ \hline  Z^TDV^{-1}BX+Z^TDWDBX+Z^TDV^{-1}QDX - \frac{1}{2} CZ^{-T}X - G^{-1}Z^{-T}X & Z^{T}DV^{-1}BZ + Z^{T}DWDBZ + Z^{T}DV^{-1}QDZ-\frac{1}{2}G^{-1}-G^{-1} \end{array}\right]$$

We also need to estimate the components of $G$ which are the variance components of our model (and of particular interest in genetics). We can attempt to maximise the 
loglihood using NR as for the $\beta$s and $b$s. It may also be useful to approximate the marginal loglihood of $b$ to evaluate when we reach convergence for the variance 
components. In a Gaussian GLMM, we can use restricted maximum likelihood to estimate the variance components, however, it's not super clear if that makes sense here because it 
involves creating a pseudo-variable by projecting the response variable $y$ into a space that is orthogonal to the fixed effects, then estimating the variance components. My 
concern with this approach is that we can't guarantee the sampling distribution of these residuals (we might? expect them to be $\sim Normal(0, \sigma))$. Again, the uncertainty 
for me here is that this involves trying to model the total variance of the model which is $Var[y|b] = E[(y - E[y|b])^2|b]$. We know what $Var[Y|b] = \frac{\mu^2}{r} - \mu$ and 
$E[y|b] = \mu$, but we don't know the full expectation of the squared deviation conditional on $b$.

The point is, this is all rather messy, so I'll use NR to estimate the variance components, and check the convergence by approximating the marginal loglihood w.r.t $b$ using a 
Laplace approximation. I'm going to heavily abuse notation here and swap between the scalar and matrix forms for the Score equations and Hessian for the variance components. 
This is largely because the Score vector only makes sense for each variance component, but the Hessian must take into account the co-variance between them.

Reminding ourselves of the scalar and matrix forms of our loglihood, only considering the terms relevant to the estimation of $\sigma$.

$$\ell(\sigma; y, \beta, b) = -(\frac{c}{2})log 2\pi - \frac{1}{2}log(2\sigma^2) - \frac{1}{2}\big({\frac{b}{\sigma}} \big)^2 \\ \ell(G; y, \beta, b) = - \frac{c}{2}log2\pi - \frac{1}{2}log|G|-\frac{1}{2}b^TG^{-1}b$$

Then we have the equation for each $\sigma_i: \ i  \in \{1, 2, ..., c\}$ in matrix form:

$$S(\sigma_i) = - \frac{1}{\sigma_i} + \frac{4b^2_i}{\sigma^3_i} \\ S(\sigma) = -\frac{1}{2}tr \big[G^{-1} \frac{\partial G}{\partial\sigma_i} \big] +\frac{1}{2}b^TG^{-1} \frac{\partial G}{\partial\sigma_i} G^{-1}b $$

It is important to note that this describes the score equation _for each variance component_. This means that we have to then compute the individual elements of the Hessian 
matrix for each pair of variance components:

$$H(\sigma_{ij}) = \frac{1}{2|G|} tr(G^{-1}\frac{\partial G}{\partial \sigma_i}) - \frac{1}{2} \bigg[tr(G^{-1}\frac{\partial G}{\partial\sigma_i})tr(G^{-1}\frac{\partial G}{\partial\sigma_j}) - tr[(G^{-1}\frac{\partial G}{\partial\sigma_i}) (G^{-1}\frac{\partial G}{\partial\sigma_j})]   \bigg] - b^TG^{-1}\frac{\partial G}{\partial\sigma_i}G^{-1}\frac{\partial G}{\partial\sigma_j}G^{-1}b$$

The final hessian matrix for the variance component estimation is then:

$$ H(\sigma) = \left[ \begin{array}{cccc} \frac{\partial S(\sigma_1)}{\partial \sigma_1} & \frac{\partial S(\sigma_2)}{\partial \sigma_1} & ... & \frac{\partial S(\sigma_j)}{\partial \sigma_1} \\ \frac{\partial S(\sigma_2)}{\partial \sigma_1} & \frac{\partial S(\sigma_2)}{\partial \sigma_2} & ... & \frac{\partial S(\sigma_j)}{\partial \sigma_2} \\ \vdots & & \ddots & \vdots \\ \frac{\partial S(\sigma_1)}{\partial \sigma_i} & \frac{\partial S(\sigma_2)}{\partial \sigma_i} & ... & \frac{\partial S(\sigma_j)}{\partial \sigma_{ij}} \end{array} \right]$$

Is there any reason _not_ to use this?

### Laplace approximation

To estimate the variance components, the common approach is to find a rotation of the model that projects the data into a space that is orthogonal to the fixed effects; this is 
essentially equivalent to regressing out the fixed effects. These rotated observations are then used to get an unbiased _restricted_ maximum likelihood estimate of the variance 
components. This probably works well in a Gaussian LMM setting because the residuals after the rotation will also be $\sim Normal(0, \sigma^2)$. However, there is no guarantee 
that this holds true when $y|b \sim NegativeBinomial$. An alternative approach is to construct a pseudo-variable $y*$ which acts similarly, however, this is not a universal fix 
and has been noted to be problematic for negative binomial distributed GLMMs. Therefore, we need to find some other way to integrate over the fixed effects to estimate the 
variance components.

In comes the Laplace approximation. This is an approach to approximate complex integrals using a Gaussian kernel. In the MLE setting, we are trying to estimate the integral 
over our fixed effect/mean terms so that we can approximate just the variance components. Therefore, we are trying to approximate the peak of our (log) likelihood function 
with a Gaussian. This makes intuitive sense if we have a uni-modal likelihood function (no guarantee!). We start with a second order Taylor series expansion of our 
loglihood function around our best estimate of our parameters.

$$\theta = [\beta \ b] \\ \ell(\sigma, \theta, y) \approx \ell(\sigma, \tilde{\theta}, y) - \frac{1}{2} (\theta - \tilde{\theta})^TH(\tilde{\theta})(\theta - \tilde{\theta}) $$

This doesn't look entirely like a second order expansion, but that is because the term with the first derivative cancels to 0 as the expansion is done around the optimum value 
$\tilde{\theta}$. Now, substituting in our actual integral:

$$\ell(\sigma, y) = log \ \int_{\Re^{k+c}} \text{exp}\big(\ell(\sigma, \tilde{\theta}, y) - \frac{1}{2}(\theta -\tilde{\theta})^TH(\tilde{\theta})(\theta - \tilde{\theta})\big) d\theta \\ = \ell(\sigma, \tilde{\theta}, y) - \frac{1}{2} \ log \ \bigg|\frac{H(\tilde{\theta})}{2\pi} \bigg| $$

We then eliminate the integral by transforming this into an integration of a multivariate normal density with $\mu = \tilde{\theta}$ and covariance $\Sigma = H^{-1}(\tilde{\theta})$. The full form is:

$$ \ell_{LA}(\sigma, y) =  n\bar{y}log \bigg[1-\frac{r}{\mu} \bigg] - nrlog\bigg[1 - \frac{r}{\mu} \bigg] + nrlogr + nrlog\mu + \sum_{i=1}^n log \bigg[\frac{\Gamma(y_i)}{\Gamma(r)} \bigg] -(\frac{c}{2})log2\pi - \frac{1}{2}log (2\sigma^2) - \frac{1}{2}(\frac{b}{\sigma})^2 \ - \frac{1}{2} \ log \ \bigg|\frac{H(\tilde{\theta})}{2\pi} \bigg|$$


### Method of moments for initial estimates

Whilst NR is generally fast, it can diverge if the starting values are not close to the (local) optima. Therefore, I want to derive some sensible starting values 
for the mean, variance and overdispersion paramaters. One approach is to use methods of moments to get initial values for NR, and then optimise them further from 
there.

Method of moments involves equating the theoretical moments with the sample moments, i.e. $E[Y] = \frac{1}{n}\sum_{i=1}^n y_i$, $E[Y^2] = \frac{1}{n}\sum_{i=1}^n y^2_i$, and solve for the relevant parameters using as many moments as there are parameters required. The mean, variance and overdispersion are dictated by 2 
parameters, so we only need 2 moments.

$$E[Y] = \frac{1}{n}\sum_{i=1}^n y_i = \mu = \frac{r}{p} \\ Var[Y] = \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2 = \xi = \frac{r(1-p)}{p^2} $$

Now we can solve for $p$ and $r$; the latter is the overdispersion parameter for our model, and $p = \frac{r}{\bar{y}}$. Now substituting this into the variance 
formula we can generate an expression for $r$ after some rearranging and manipulation.

$$Var[Y] = \frac{r(1-\frac{r}{\bar{y}})}{(\frac{r}{\bar{y}})^2} = \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2  \\ r = \frac{\bar{y}^2}{\frac{1}{n-1}\sum_{i=1}^n(y_i - \bar{y})^2 + \bar{y}}$$

## A methods of moments-like approach to variance component estimation

__NB__(10/11/2021) - Using NR to estimate the variance components with the ANOVA VCs as initial values is very unstable and leads to infinite estimates. After 
talking to Shila we (she) worked through a potential approach to estimate the variance components at each NR iteration for the linear predictor coefficients.

We remind ourselves of a few facts about our GLMM first:

$$Y|b \sim NegativeBinomial(\frac{\mu}{r}, r) \\ b \sim Normal(0, \sigma^2) \\ E[Y|b] = \mu \\ Var[Y|b] = \xi = \frac{\mu^2}{r} - \mu \\ \eta = X\beta + Zb \\ \mu = e^\eta \\ Var[\mu] = \frac{\mu^2}{r} - \mu$$

We are going to use a series of attributes of variance and expecations to finally achieve a solution for our $\sigma^2$ variance components. We also need to remind 
ourselves of the law of total variances (LOTV) and law of iterated expectations (LIE):

$$ Var[Y] = E[Var[Y|b]] + Var[E[Y|b]] $$

Recalling that $E[Y|b] = \mu$ is a random variable i.e., we can take it's expectation and variance, and that $r \in \mathbb{R}$.

$$ Var[Y|b] = \frac{\mu^2}{r} - \mu \\ = \frac{e^{2\eta}}{r} - e^{\eta} \\ = \frac{e^{2X\beta + 2Zb}}{r} - e^{X\beta + Zb}$$

$b$ is random variable as defined above.

$$E[Var[Y|b]] = E[\frac{e^{2X\beta + 2Zb}}{r} - e^{X\beta + Zb}] \\ = \frac{1}{r} \big\{ E[e^{2X\beta + 2Zb}] - rE[e^{X\beta+Zb}]  \big\} \ \text{We use the linear properties of expectations here} \\ =\frac{1}{r} \big\{e^{2X\beta}E[e^{2Zb}-re^{X\beta}E[e^{Zb}]   \big\} \ \text{treat X}\beta\ \text{as a constant} \\ =\frac{1}{r} \big\{ e^{2X\beta}-re^{X\beta} \big\} \text{Using E[b] = 0, then} \ E[e^{Zb}]=1$$

Now, we want $Var[E[Y|b]]$. Noting that $E[Y|b] = \mu =e^{X\beta + Zb} = e^{X\beta}e^{Zb}$, we can then define $Var[E[Y|b]]$:

$$Var[E[Y|b]] = Var(e^{X\beta + Zb}) \\ = Var(e^{X\beta}e^{Zb}) \\ = (e^{X\beta})^2Var(e^{Zb}) \ \text{Using } Var(aX) = a^2Var(X) \\ =e^{2X\beta}Var(e^{Zb})$$

We know that $b \sim Normal(0, \sigma^2)$ and we can use the fact that the MGF of a normal distribution is $E[e^{tZ}] = exp(\mu t + \frac{\sigma^2 t^2}{2})$ from https://stats.stackexchange.com/questions/89970/exponential-of-a-standard-normal-random-variable. Now Let $f(b) = e^{Zb}$ then:

$$Var(f(b)) = E(f(b)^2) - E(f(b))^2 \\ =E[e^{2Zb}] - [E(e^{Zb})]^2 \\ = exp(\mu 2Z + \frac{\sigma^2 (2Z)^2}{2}) - [exp(\mu Z + \frac{\sigma^2}{2})]^2 \ \text{Note that } \mu=E[b]=0 \\ = exp(0 + 2Z^2\sigma^2) - exp(0 + \sigma^2Z^2) \\ Var(e^{Zb}) = exp(\sigma^2 Z^2) [e^{\sigma^2 Z^2} - 1] $$

Therefore, $Var[E[Y|b]] = e^{2X\beta} Var[e^{Zb}] = e^{2X\beta} e^{\sigma^2 Z^2}(e^{\sigma^2Z^2} - 1)$. Now we can write out an expression for $Var[Y]$:

$$Var[Y] = E[Var[Y|b]] + Var[E[Y|b]] \\ = \frac{1}{r}[e^{2X\beta} - re^{X\beta}] + e^{2X\beta}e^{\sigma^2 Z^2}(e^{\sigma^2 Z^2} - 1) \\ \hat{Var[Y]} = S^2$$

Now we can equate $\hat{Var[Y]} = S^2$ (our sample variance), and we want to solve for $\sigma^2$, which will require the quadratic formula, and we will take the 
logarithm of the positive root. Recall the quadratic formula:

$$\text{Where} \ 0 = ax^2 +bx + c \\ x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} $$

For our expression above we can equate the sample variance with our expression for $Var[Y]$ and with a bit of trivial rearranging write this in terms of a quadratic.


$$a=e^{2X\beta}, \ b= -e^{2X\beta}, \ c=\frac{1}{r}[e^{2X\beta} - re^{X\beta}] - S^2, \ x=e^{\sigma^2 Z^2} \\ 0 = ax^2 + bx + c \\ 0 = e^{2X\beta} (e^{\sigma^2 Z^2})^2 + (-)e^{2X\beta} e^{\sigma^2 Z^2} + \frac{1}{r}[e^{2X\beta} - re^{X\beta}] - S^2 \\ x = \frac{-(-)e^{2X\beta} \pm \sqrt{((-)e^{2X\beta})^2 - 4(e^{2X\beta}[ \frac{1}{r}[e^{2X\beta} - re^{X\beta}] - S^2]) }  }{2e^{2X\beta}}$$

From the 2 solutions from this we take the positive value as $x=e^{\sigma^2 Z^2}$, and solve for $\sigma^2$:

$$\sigma^2 = \frac{logx}{Z^2} $$

If neither value is positive we might need to constrain $\sigma^2$ to 0. As $Z$ is a matrix, we might also need to compute this element-wise, though the final 
$\sigma^2$ should only take on a single value (or a vector if there is > 1 random effect).

__NB__(Update:12/11/21) - this approach gives a solution for _each observation_, whereas we want the variance components _across_ observations. This means I 
either need to use the grand mean in place of the estimated mean (intercept?), or I need a different approach.

### Mike's appalling shonky ANOVA-like approximation (MASALA)

I've been thinking about how to do this in an ANOVA/methods of moments-like approach. I think I can use the estimated fixed and random effect coefficients to 
construct an ANOVA table to get values for the expected mean squares, which can then be used to solve for the variance components.

The algorithm proceeds as follows:


* Compute initial estimates for $\theta_0 = [\{\beta_1, \beta_2, ..., \beta_i\} \ \{b_1, b_2, ..., b_j \} \ \{\sigma_1^2, \sigma_2^2, ..., \sigma_j^2  \}]$ as the vector of all parameters
* While $converge = False$:
    * $k \mathrel{+}= 1$
    * Estimate $\beta$ and $b$ using NR
    * Compute ANOVA table quantities: group means, mean squares, degrees of freedom and expected mean squares
        - Fixed effect group means $= \beta_0 + \beta_i$
        - Random effect group means are the random effect coefficients i.e., $b_j$
        - Use these group means to compute sums of squares for each fixed/random effect variable
        - Calculate the mean squares as the SS/df
    * Construct a linear system of equations: $MS = M\xi$
        - $MS$ is a vector of the mean squares for all variables and the residual variance
        - $M$ is a design matrix that contains the entries of the expected mean square coefficients
        - $\xi$ is the vector of variance components to be estimated
    * Solve this linear system using a Cholesky decomposition with back(forward) substitution (R `chol` returns $U$ so we do back substitution).
* Set $\theta_k = [\{\beta_1, \beta_2, ..., \beta_i\} \ \{b_1, b_2, ..., b_j \} \ \{\sigma_1^2, \sigma_2^2, ..., \sigma_j^2  \}]$
* Compute extended loglihood $\ell^{(k)}(\theta_k)$ using $\theta_k$
* Evaluate $|\theta_k - \theta_{k-1}| \leq \epsilon$ where $\epsilon$ is an arbitrary small number. 
* Evaluate $|\ell^{(k)} - \ell^{(k-1)}| \leq \epsilon_\ell$ where $\epsilon_\ell$ is another arbitrarily small number.
* If both statement are true then model has converged, else $k \mathrel{+}= 1$ and repeat

The downside to this approach is that there is no way to estimate a standard error for the $\sigma^2$'s without bootstrapping. We can still do inference on the fixed 
effects, which is _generally_ our main interest, as we have the Hessian. We can also do likelihood-based inference as we have a loglihood that we have tried to 
(locally) maximise.

Concretely, the steps for the variance component approximations are as follows.

Define the vector of mean squares $(MS)$:

$$MS = [MS\beta \ MSb]^T \\ MS\beta = [MS\beta_1 \ MS\beta_2 \dots MS\beta_i]^T = [\frac{SS\beta_1}{df_1} \ \frac{SS\beta_2}{df_2} \dots\ \frac{SS\beta_i}{df_i} ] \\ MSb = [MSb_1 \ MSb_2 \dots MSb_j]^T = [\frac{SSb_1}{df_1} \ \frac{SSb_2}{df_2} \dots\ \frac{SSb_j}{df_j} ]$$

Each entry of the $MS$ vector is computed as the sum of squares divided by the degrees of freedom for the variable in question:

$$SS\beta_i = (e^{\beta_0 + \beta_i} - \bar{y})^2 \\  SSb_j = (\sum^q_{l=1} e^{b_{jl}} - \bar{y})^2$$

$e^{\beta_0 + \beta_i}$ is the group mean for fixed effect variable $i$ and $e^{b_{jl}}$ is the group mean for the $l \in \{1, 2, \dots, q\}$ level of random effect 
variable $j$.  The degrees of freedom at the number of levels - 1 for factor variables and 1 for continuous variables. This gets more complicated for interaction 
term variables, but we won't consider them here for now.

We then need to calculate the expected mean squares (EMS) as for an ANVOA table, except we are going to write this as product of a matrix of coefficients and a 
vector of parameters to solve for:

$$M = \left[ \begin{array}{cccc|cccc|c} FE_1 & 0 & 0 & \dots & \dots & \dots & \dots & \dots & 1 \\ 0 & FE_2 & 0 & \dots & \dots & \dots & \dots & \dots & 1 \\ \vdots & 0 & \ddots & \dots & \dots & \dots & \dots & \dots & 1 \\ 0 & \dots & \dots & FE_i & \dots & \dots & \dots & \dots & 1 \\ 0 & \dots & \dots & \dots & RE_1 & \dots & \dots & \dots & 1 \\ 0 & \dots & \dots & \dots & \dots & RE_2 & \dots & \dots & 1 \\ 0 & \dots & \dots & \dots & \dots & \dots & \ddots & \dots & 1 \\0 & \dots & \dots & \dots & \dots & \dots & \dots & RE_j & 1  \end{array} \right] $$

The diagonal entries of $M$ contain the coefficients which are the products of the number of variables levels i.e. the same ones used to compute the degrees of 
freedom. This is pretty straight forward to calculate when no levels are shared - this changes for interaction terms, so I won't include them for now, but 
essentially it just means the degrees of freedom will need to be adjusted accordingly, and the coefficients here will take into account the numbers of levels of 
_both_ main variables.

Now we can set up our linear system of equations, and solve it using a Cholesky decomposition:

$$MS = M\theta \\ M = LL^T \\ MS = LL^T\theta \\ y = L^T\theta \\ \text{Solve } Ly= MS \\ \text{Solve } L^T\theta =y$$

We solve for $y$ using forward substitution and then solve for $\theta$ with back substitution. This gives us our estimates of the parameters, and we take the 
random effect variance components as our new $\sigma^2$ values.


## Computing a dispersion term

To shoehorn our model into a GLM(M) we had to treat the negative binomial dispersion parameter as a known value. In reality that means we need some way to either compute it based 
on an exact expression, or to approximate by some means. I've opted for the former because it will be quicker, but it might not be as accurate.

We start with the law of total variance (LOTV) and a few identities:

$$ Var[Y] = E[Var(Y|b)] + Var[E(Y|b)] \\ E[Y|b] = \mu \\ Var[Y|b] = \frac{\mu^2}{r} - \mu$$

Using these identities, we can re-write the law of total variance in terms of expectations:

$$Var[Y] = E[\frac{\mu^2}{r} - \mu] + Var[\mu] \\ Var[Y] = E[\frac{\mu^2}{r} - \mu] + E[\mu^2] - E[\mu]^2 \\ Var[Y] = \frac{1}{r}(E[\mu^2 - r\mu]) + E[\mu^2] - E[\mu]^2 \\ Var[Y] = \frac{1}{r}(E[\mu^2] - rE[\mu]) + E[\mu^2] - E[\mu]^2$$

So we have $Var[Y]$ written in terms of the expectations of $\mu$, great. We can treat $\mu$ as a random variable, which then means that it will have expected values and moments, 
assuming that these are defined. So we are going to assume that, and we are then going to assume that the _sample_ moments are good approximations for the _actual_ moments of $\mu$ 
and substitute these into our expression.

$$E[\mu] = \frac{1}{n} \sum_{i=1}^n \mu_i \\ E[\mu^2] = \frac{1}{n} \sum_{i=1}^n \mu_i^2$$

We will also assume that the sample variance of $Y$ is a good approximation to $Var[Y]$, such that $S^2 =\frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 = \hat{Var[Y]} \approx Var[Y]$. 
Putting this together we can write an expression in terms of $r$:


$$ S^2 = \frac{1}{r}(E[\mu^2] - rE[\mu]) + E[\mu^2] - E[\mu]^2 \\ S^2 = \frac{1}{r}(\frac{1}{n} \sum_{i=1}^n \mu_i^2 - r\frac{1}{n} \sum_{i=1}^n \mu_i) + \frac{1}{n}\sum_{i=1}^n \mu_i^2 - (\frac{1}{n}\sum_{i=1}^n \mu_i)^2 \\ S^2 = \frac{1}{r}(\frac{1}{n} \sum_{i=1}^n \mu_i^2) - \frac{1}{n} \sum_{i=1}^n \mu_i + \frac{1}{n} \sum_{i=1}^n \mu_i^2 - (\frac{1}{n} \sum_{i=1}^n \mu_i)^2 \\ S^2 = \frac{1}{n} \sum_{i=1}^n \mu_i^2 (\frac{1}{r} + 1) - \frac{1}{n} \sum_{i=1}^n \mu_i(1 + \frac{1}{n} \sum_{i=1}^n \mu_i)$$

We can now rearrange this expression in terms of $r$:

$$\frac{1}{r} = \frac{S^2 + \frac{1}{n} \sum_{i=1}^n \mu_i(1 + \frac{1}{n} \sum_{i=1}^n \mu_i)}{\frac{1}{n} \sum_{i=1}^n \mu_i^2} - 1 \\ r = \frac{\frac{1}{n} \sum_{i=1}^n \mu_i^2}{S^2 + \frac{1}{n} \sum_{i=1}^n \mu_i(1 + \frac{1}{n} \sum_{i=1}^n \mu_i)} - 1 $$

Et voila!

<!---

## Joint parameter estimation

What about trying to jointly maximise all of our parameters of interest?

$$S(\beta) = \frac{\partial \ell(\theta; y, r)}{\partial \beta} \\ S(b) = \frac{\partial \ell(\theta; y, r)}{\partial b} \\ S(\sigma) = \frac{\partial\ell(\theta;y, r)}{\partial\sigma} \\ H(\beta)=\frac{\partial^2\ell(\theta; y, r)}{\partial\beta_j\partial\beta_k} \\ H(b)=\frac{\partial^2\ell(\theta; y, r)}{\partial b_l \partial b_q} \\ H(\sigma) = \frac{\partial^2\ell(\theta;y, r)}{\partial\sigma_i\partial\sigma_j}$$

To compute these involves finding the various partial derivatives, and extensive use of the chain rule. I wont give all of the calculations as they are laborious, but I'll 
provide some of the symbol calculations to aid working backwards from each to $\ell(\theta; y, r)$. Using $\ell(\theta)$ as a short cut to $\ell(\theta; y, r)$:

$$S(\beta) = \frac{\partial \ell(\theta)}{\partial \theta} \frac{\partial \theta}{\partial \mu} \frac{\partial \mu}{\partial \eta} \frac{\partial \eta}{\partial \beta} \\ S(b) = \frac{\partial \ell(\theta)}{\partial \theta} \frac{\partial \theta}{\partial \mu} \frac{\partial \mu}{\partial \eta} \frac{\partial \eta}{\partial b} \\ S(\sigma)=\frac{\partial\ell(\theta)}{\partial\theta} \frac{\partial\theta}{\partial\sigma}$$

For convenience we have some of the easier partial derivatives in (mostly) scalar form, based on a number of identities:

$$\eta=X\beta + Zb \\ \frac{\partial \eta}{\partial \beta} = X \\ \frac{\partial \eta}{\partial b} = Z \\ \mu = e^\eta \\ \frac{\partial \mu}{\partial\eta} = e^\eta=\mu \\ \theta = 1- r\mu^{-1} \\ \frac{\partial \theta}{\partial \mu} = Var(\mu)^{-1} = Var[Y]^{-1}=(\frac{\mu^2}{r} - \mu)^{-1} \\ \frac{\partial \ell(\theta)}{\partial \theta} = y - \frac{\frac{\partial b(\theta)}{\partial \theta}}{a(\phi)} = y - \frac{\partial b(\theta)}{\partial \theta} \\ \frac{\partial b(\theta)}{\partial \theta} = n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)$$

Because we are invariably using vectors of parameters, where a scalar is a 1-component vector, we can write some of these in matrix notation:

$$\frac{\partial \mu}{\partial\eta} = \underset{n\times n}{diag}(\mu_i) = D \\ \frac{\partial \theta}{\partial \mu} = \underset{n \times n}{diag}(\frac{\mu_{i}^2}{r} - \mu_{i})^{-1} = V^{-1}$$

That is, $D$ is a diagonal matrix of the individual means $\mu_i$ and $V$ is a diagonal matrix of the variances, and $V^{-1}$ is its inverse. Putting all of these components 
together with $\frac{\partial \ell(\theta)}{\partial \theta}$ leads to the following score equation vectors:

$$S(\beta) = X^TDV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)] \ : \ k \times 1 \ \text{vector} \\ S(b) = Z^{T}DV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)] - \frac{1}{2}G^{-1}b - G^{-1}b : \ c \times 1 \ \text{vector} \\ S(\sigma_i) = -\frac{1}{2}tr \big[G^{-1} \frac{\partial G}{\partial\sigma_i} \big] +\frac{1}{2}b^TG^{-1} \frac{\partial G}{\partial\sigma_i} G^{-1}b$$

Here $G=Var(b)$, the covariance of $b$ which needs to be estimated, i.e. $\sigma$. We also need to compute $\frac{\partial G}{\partial\sigma_i}$, which is simply the single 
entry matrix $J^{ij}$ because we are in essence computing $\frac{\partial G}{\partial G_{ij}}$.

Taking the second derivatives we find the Hessian matrices for $\beta$:

$$H(\beta) = X^TDV^{-1}BX + X^TDWDBX + X^TDV^{-1}QDX : \ k \times k \ \text{matrix} \\ W=\frac{\partial V^{-1}}{\partial \mu} =  \underset{n \times n}{diag}\big(\frac{\mu^2(1-2\mu)}{r} - \frac{2\mu^3}{r^2} - \mu\big) \\ B = \underset{n \times n}{diag}(y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)) \\  Q=\underset{n \times n}{diag} (-n\big(1 + \frac{r^2}{\mu^2(1-r\mu^{-1})^2} \big))$$

and for $b$:

$$H(b) =  Z^{T}DV^{-1}BZ + Z^{T}DWDBZ + Z^{T}DV^{-1}QDZ-\frac{1}{2}G^{-1}-G^{-1} : \ c \times c \ \text{matrix} \\ W=\frac{\partial V^{-1}}{\partial \mu} =  \underset{n \times n}{diag}\big(\frac{\mu^2(1-2\mu)}{r} - \frac{2\mu^3}{r^2} - \mu \big) \\ B = \underset{n \times n}{diag}(y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)) \\ Q=\underset{n \times n}{diag} (-n\big(1 + \frac{r^2}{\mu^2(1-r\mu^{-1})^2} \big))$$

the same for $\sigma$:

$$H(\sigma_{ij}) = \frac{1}{2|G|} - \frac{1}{2} \bigg[tr(G^{-1}\frac{\partial G}{\partial\sigma_i})tr(G^{-1}\frac{\partial G}{\partial\sigma_j}) - tr[(G^{-1}\frac{\partial G}{\partial\sigma_i}) (G^{-1}\frac{\partial G}{\partial\sigma_j})]   \bigg] - b^TG^{-1}\frac{\partial G}{\partial\sigma_i}G^{-1}\frac{\partial G}{\partial\sigma_j}G^{-1}b$$

Therefore, each step of the NR iteration is:

$$\hat\beta = \tilde\beta - H(\beta)^{-1}S(\beta) \\ \hat\beta = \tilde\beta - [X^TDV^{-1}BX + X^TDWDBX + X^TDV^{-1}QDX]^{-1} \bigg[X^TDV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)]\bigg]\\ \hat{b} = \tilde b-H(b)^{-1}S(b) \\ \hat{b} = \tilde b - [Z^{T}DV^{-1}BZ + Z^{T}DWDBZ + Z^{T}DV^{-1}QDZ-\frac{1}{2}G^{-1}-G^{-1}]^{-1}\bigg[ Z^{T}DV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)] - \frac{1}{2}G^{-1}b - G^{-1}b \bigg] \\ \hat{\sigma_i} = \tilde{\sigma_i} - H(\sigma_{ij})^{-1}S(\sigma_i) $$

I have deliberately omitted the actual matrix forms for sigma because we have to estimate the individual variance components, i.e. we don't have a nice closed-form convention to 
write this without significantly abusing notation.







We can also define the full Hessian for $\theta = [\beta \ b \ \sigma]$, which I think might mean that we can compute the updates jointly. Therefore, we can define the full 
vector of score equations and Hessian as:

$$\theta = [\beta \ b \ \sigma] \\ S(\theta) = \begin{bmatrix} S(\beta) \\ S(b) \\ S(\sigma) \end{bmatrix} \\ H(\theta) = \left[ \begin{array}{c|c|c} \frac{\partial S(\beta)}{\partial \beta} \ \frac{\partial S(\beta)}{\partial b} \ \frac{\partial S(\beta)}{\partial \sigma} \\ \frac{\partial S(b)}{\partial \beta} \ \frac{\partial S(b)}{\partial b} \ \frac{\partial S(b)}{\partial \sigma} \\ \frac{\partial S(\sigma)}{\partial \beta} \ \frac{\partial S(\sigma)}{\partial b} \ \frac{\partial S(\sigma)}{\partial \sigma} \end{array} \right]$$

This full Hessian matrix needs a few more matrices to be defined:

$$ M = \underset{c \times c}{diag}((\frac{\partial\sigma}{\partial b})^{-1}) = \underset{c \times c}{diag} \big(\frac{c(\frac{1}{c}\sum^c_{j=1} u^2_{j})^{\frac{1}{2}}}{b} \big) \\ U = \underset{c \times c}{diag}(b) $$

Writing this out in full: 

$$\theta = [\beta \ b \ \sigma] \\ S(\theta) = \begin{bmatrix} X^TDV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)] \\ Z^{T}DV^{-1}[y_i - n\big(\mu - \frac{r}{1-r\mu^{-1}}\big)] - \frac{1}{2}G^{-1}b - G^{-1}b \\ \frac{1}{2}G^{-1} - \frac{1}{2}bb^T(G^2)^{-1} \end{bmatrix} \\ H(\theta) = \left[\begin{array}{c|c|c} X^TDV^{-1}BX + X^TDWDBX + X^TDV^{-1}QDX & X^TDV^{-1}BZ + X^TDWDBZ + X^TDV^{-1}QDZ & X^TDV^{-1}BZM + X^TDWDBZM + X^TDV^{-1}QDZM \\ \hline  Z^TDV^{-1}BX+Z^TDWDBX+Z^TDV^{-1}QDX & Z^{T}DV^{-1}BZ + Z^{T}DWDBZ + Z^{T}DV^{-1}QDZ-\frac{1}{2}G^{-1}-G^{-1} & Z^TDV^{-1}BZM+Z^TDWDBZM+Z^TDV^{-1}QDZM - \frac{1}{2}U(G^2)^{-1}) \\ \hline (\frac{1}{2}(G^2)^{-1} - bb^TG^{-1}M^{-1})Z^{-T}X & \frac{1}{2}(G^2)^{-1}-bb^TG^{-1}M^{-1} & \frac{1}{2}(G^2)^{-1} - bb^T G^{-1} \end{array}\right]$$

--->





